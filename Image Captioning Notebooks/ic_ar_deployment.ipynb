{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "ic_ar_sami_deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LeyWXRVUepFZ",
        "FbmzfaZMfNvH",
        "zJSqeGE2fVZI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Link Drive & import libs"
      ],
      "metadata": {
        "id": "iSGa39S3eUoL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erQ_EoC_N-Qy",
        "outputId": "1fbe8220-aeac-45bb-91aa-d4e3555037c0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "XIVgxVGFNk16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import os\r\n",
        "import time\r\n",
        "import json\r\n",
        "import pickle\r\n",
        "import random\r\n",
        "import zipfile\r\n",
        "import collections\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "metadata": {
        "id": "q42lVMKYNrVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init base path (for selecting Model)"
      ],
      "metadata": {
        "id": "rlJGO_XmN1y3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# max_length = 49  # maximum length of any caption in MS-COCO\r\n",
        "# m_path = os.path.abspath('drive/MyDrive/model_data_MS_E35_B256')\r\n",
        "\r\n",
        "max_length = 28  # maximum length of any caption in Flickr8k\r\n",
        "m_path = os.path.abspath('drive/MyDrive/model_data_Flick_E35_B128')\r\n",
        "\r\n",
        "print('Base path:', m_path)\r\n",
        "# Ex: C:\\Users\\Sami\\Faculty of Information Technology\\Fifth_Year"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base path: /content/drive/MyDrive/model_data_Flick_E35_B128\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqQXnyy5N1Wn",
        "outputId": "1c2cb246-b9ab-4e36-a377-01874c38286e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model components"
      ],
      "metadata": {
        "id": "LeyWXRVUepFZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "def load_image(image_path):\r\n",
        "    img = tf.io.read_file(image_path)\r\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\r\n",
        "    img = tf.image.resize(img, (299, 299))\r\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\r\n",
        "    return img, image_path"
      ],
      "outputs": [],
      "metadata": {
        "id": "5v9B97u8Tx_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the images using InceptionV3"
      ],
      "metadata": {
        "id": "ld8OqMdjT5oH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\r\n",
        "new_input = image_model.input\r\n",
        "hidden_layer = image_model.layers[-1].output\r\n",
        "\r\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "outputs": [],
      "metadata": {
        "id": "yMZQwTPST6Yk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Choose the top 5000 words from the vocabulary\r\n",
        "top_k = 10000\r\n",
        "if os.path.isdir(m_path + '/model_data') and os.path.isfile(m_path + '/model_data/tokenizer.pickle'):\r\n",
        "  with open(m_path + '/model_data/tokenizer.pickle', 'rb') as handle:\r\n",
        "    tokenizer = pickle.load(handle)\r\n",
        "    tokenizer.word_index['<pad>'] = 0\r\n",
        "    tokenizer.index_word[0] = '<pad>'\r\n",
        "    print('Tokenizer is loaded!')\r\n",
        "else:\r\n",
        "  raise Exception('Couldn\\'t find Tokenizer file at ' + m_path + '/model_data/tokenizer.pickle')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer is loaded!\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWscYhazVCys",
        "outputId": "2268fcf0-2a4d-4ccd-820b-5590e7bf7338"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a tf.data Dataset for training ($)"
      ],
      "metadata": {
        "id": "dpvusjTVfaGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "embedding_dim = 256\r\n",
        "units = 512\r\n",
        "vocab_size = top_k + 1\r\n",
        "\r\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\r\n",
        "attention_features_shape = 64"
      ],
      "outputs": [],
      "metadata": {
        "id": "--tvgNUDfqRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "0XcZGS6ohPYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "class BahdanauAttention(tf.keras.Model):\r\n",
        "  def __init__(self, units):\r\n",
        "    super(BahdanauAttention, self).__init__()\r\n",
        "    self.W1 = tf.keras.layers.Dense(units)\r\n",
        "    self.W2 = tf.keras.layers.Dense(units)\r\n",
        "    self.V = tf.keras.layers.Dense(1)\r\n",
        "\r\n",
        "  def call(self, features, hidden):\r\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\r\n",
        "\r\n",
        "    # hidden shape == (batch_size, hidden_size)\r\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\r\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\r\n",
        "\r\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\r\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\r\n",
        "                                         self.W2(hidden_with_time_axis)))\r\n",
        "\r\n",
        "    # score shape == (batch_size, 64, 1)\r\n",
        "    # This gives you an unnormalized score for each image feature.\r\n",
        "    score = self.V(attention_hidden_layer)\r\n",
        "\r\n",
        "    # attention_weights shape == (batch_size, 64, 1)\r\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\r\n",
        "\r\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\r\n",
        "    context_vector = attention_weights * features\r\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\r\n",
        "\r\n",
        "    return context_vector, attention_weights"
      ],
      "outputs": [],
      "metadata": {
        "id": "hSSN1v6chTSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "class CNN_Encoder(tf.keras.Model):\r\n",
        "    # Since you have already extracted the features and dumped it\r\n",
        "    # This encoder passes those features through a Fully connected layer\r\n",
        "    def __init__(self, embedding_dim):\r\n",
        "        super(CNN_Encoder, self).__init__()\r\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\r\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\r\n",
        "\r\n",
        "    def call(self, x):\r\n",
        "        x = self.fc(x)\r\n",
        "        x = tf.nn.relu(x)\r\n",
        "        return x"
      ],
      "outputs": [],
      "metadata": {
        "id": "z0T4rs7ZhU0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "class RNN_Decoder(tf.keras.Model):\r\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\r\n",
        "    super(RNN_Decoder, self).__init__()\r\n",
        "    self.units = units\r\n",
        "\r\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\r\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\r\n",
        "                                   return_sequences=True,\r\n",
        "                                   return_state=True,\r\n",
        "                                   recurrent_initializer='glorot_uniform')\r\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\r\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    self.attention = BahdanauAttention(self.units)\r\n",
        "\r\n",
        "  def call(self, x, features, hidden):\r\n",
        "    # defining attention as a separate model\r\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\r\n",
        "\r\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\r\n",
        "    x = self.embedding(x)\r\n",
        "\r\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\r\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\r\n",
        "\r\n",
        "    # passing the concatenated vector to the GRU\r\n",
        "    output, state = self.gru(x)\r\n",
        "\r\n",
        "    # shape == (batch_size, max_length, hidden_size)\r\n",
        "    x = self.fc1(output)\r\n",
        "\r\n",
        "    # x shape == (batch_size * max_length, hidden_size)\r\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\r\n",
        "\r\n",
        "    # output shape == (batch_size * max_length, vocab)\r\n",
        "    x = self.fc2(x)\r\n",
        "\r\n",
        "    return x, state, attention_weights\r\n",
        "\r\n",
        "  def reset_state(self, batch_size):\r\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "outputs": [],
      "metadata": {
        "id": "-0BSudcPhXlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model (for selecting a specified epoch)"
      ],
      "metadata": {
        "id": "CQXhU-nvhdPa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "enc_dec_num = '34'\r\n",
        "\r\n",
        "if os.path.isfile(f'{m_path}/model_data/encoder{enc_dec_num}.weights.index') and os.path.isfile(f'{m_path}/model_data/decoder{enc_dec_num}.weights.index'):\r\n",
        "  encoder = CNN_Encoder(embedding_dim)\r\n",
        "  decoder = RNN_Decoder(embedding_dim, units, vocab_size)\r\n",
        "  encoder.load_weights(f'{m_path}/model_data/encoder{enc_dec_num}.weights')\r\n",
        "  decoder.load_weights(f'{m_path}/model_data/decoder{enc_dec_num}.weights')\r\n",
        "  print('encoder/decode are loaded!')\r\n",
        "else:\r\n",
        "  raise Exception(f'encoder{enc_dec_num}/decoder{enc_dec_num} not found!')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder/decode are loaded!\n"
          ]
        }
      ],
      "metadata": {
        "id": "ZSvZ43JihfId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b26beb-cce7-4e93-a4d0-fec18dddc364"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continue Loading Model Components"
      ],
      "metadata": {
        "id": "FbmzfaZMfNvH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "outputs": [],
      "metadata": {
        "id": "q8WncDZNksjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoint ($)"
      ],
      "metadata": {
        "id": "-Qgl8QSQkxVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "if not os.path.isdir(m_path + '/model_data'):\r\n",
        "  raise Exception('model_data folder is missing!')\r\n",
        "checkpoint_path = m_path + \"/model_data/checkpoints\"\r\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\r\n",
        "                           decoder=decoder,\r\n",
        "                           optimizer=optimizer)\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=50)"
      ],
      "outputs": [],
      "metadata": {
        "id": "q17oBK5claS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "last_epoch = 0\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "  print(f'last checkpoint {ckpt_manager.latest_checkpoint}')\r\n",
        "  last_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\r\n",
        "  # restoring the latest checkpoint in checkpoint_path\r\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "print('starting model from epoch:', last_epoch)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last checkpoint /content/drive/MyDrive/model_data_Flick_E35_B128/model_data/checkpoints/ckpt-35\n",
            "starting model from epoch: 35\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8lMB4kAl0Y1",
        "outputId": "02ff30cb-e61d-4f07-8570-b7584a2fcb40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "YDfK1mh1s4SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing utils"
      ],
      "metadata": {
        "id": "zJSqeGE2fVZI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "def evaluate(image):\r\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\r\n",
        "\r\n",
        "    hidden = decoder.reset_state(batch_size=1)\r\n",
        "\r\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\r\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\r\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\r\n",
        "                                                 -1,\r\n",
        "                                                 img_tensor_val.shape[3]))\r\n",
        "\r\n",
        "    features = encoder(img_tensor_val)\r\n",
        "\r\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\r\n",
        "    result = []\r\n",
        "\r\n",
        "    for i in range(max_length):\r\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\r\n",
        "                                                         features,\r\n",
        "                                                         hidden)\r\n",
        "\r\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\r\n",
        "\r\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\r\n",
        "        result.append(tokenizer.index_word[predicted_id])\r\n",
        "\r\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\r\n",
        "            return result, attention_plot\r\n",
        "\r\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\r\n",
        "\r\n",
        "    attention_plot = attention_plot[:len(result), :]\r\n",
        "    return result, attention_plot"
      ],
      "outputs": [],
      "metadata": {
        "id": "5GCh0YOFs7tl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "def plot_attention(image, result, attention_plot):\r\n",
        "    temp_image = np.array(Image.open(image))\r\n",
        "\r\n",
        "    fig = plt.figure(figsize=(10, 10))\r\n",
        "\r\n",
        "    len_result = len(result)\r\n",
        "    for i in range(len_result):\r\n",
        "        temp_att = np.resize(attention_plot[i], (8, 8))\r\n",
        "        grid_size = max(np.ceil(len_result/2), 2)\r\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i+1)\r\n",
        "        ax.set_title(result[i])\r\n",
        "        img = ax.imshow(temp_image)\r\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\r\n",
        "\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "CgFTp-CQs_98"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "def predict_url_image(image_url):\r\n",
        "  image_name_extension = image_url.split('/')[-1]\r\n",
        "  print(image_name_extension)\r\n",
        "  image_path = tf.keras.utils.get_file(image_name_extension, origin=image_url, cache_subdir='/content')\r\n",
        "  result, attention_plot = evaluate(image_path)\r\n",
        "  return image_path, result, attention_plot"
      ],
      "outputs": [],
      "metadata": {
        "id": "RC8DcWbyt2HS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on any image"
      ],
      "metadata": {
        "id": "nMQ0F398ticz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "# url = 'https://ichef.bbci.co.uk/news/873/cpsprodpb/2F0D/production/_118054021_1afcbf08-6b32-4d75-9cca-783d61d7b94f.jpg'\r\n",
        "# url = 'https://www.esafety.gov.au/sites/default/files/2019-08/Remove%20images%20and%20video.jpg'\r\n",
        "# url = 'https://i2.wp.com/digital-photography-school.com/wp-content/uploads/2016/02/Headshot-Photography-London-1052.jpeg'\r\n",
        "# url = 'https://i.chzbgr.com/original/3933445/h5ABCC7A5/just-20-random-animal-photos-that-will-make-you-scratch-your-head'\r\n",
        "# url = 'https://image.shutterstock.com/image-photo/action-hero-muscled-man-holding-260nw-196778930.jpg'\r\n",
        "# url = 'https://d28m5bx785ox17.cloudfront.net/v1/img/_E0oIsccZlmMAfrWZNVRGl4yaIYZRD2RiHCXgJFMIjc=/d/l'\r\n",
        "# url = 'https://images.unsplash.com/photo-1496181133206-80ce9b88a853'\r\n",
        "# url = 'https://thumbor.forbes.com/thumbor/fit-in/x/https://www.forbes.com/advisor/wp-content/uploads/2021/06/inground-pool.jpeg'\r\n",
        "# url = 'https://riverdalepress.com/uploads/original/1470848884_42a8.jpg'\r\n",
        "# url = 'https://bestsellingcarsblog.com/wp-content/uploads/2019/06/Shanghai-Street-3.jpg'\r\n",
        "# url = 'https://i.pinimg.com/originals/d0/0d/bd/d00dbde1ddf6ff4060aa183600900f8b.jpg'\r\n",
        "url = 'https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/kitchen-decor-ideas-1580491833.jpg?crop=1.00xw:0.669xh;0,0.151xh&resize=640:*'\r\n",
        "# url = 'https://c8.alamy.com/comp/X7M18P/lecturer-and-students-in-a-university-amphitheatre-classroom-X7M18P.jpg'"
      ],
      "outputs": [],
      "metadata": {
        "id": "GNgEEAN9f_SJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "image_path, result, attention_plot = predict_url_image(url)\r\n",
        "print('Prediction Caption:', ' '.join(result))\r\n",
        "plot_attention(image_path, result, attention_plot)\r\n",
        "# opening the image\r\n",
        "# Image.open(image_path)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "OzSdUnvOuFAd",
        "outputId": "b27fee95-0d9c-4f45-8ca3-5e833b32eace"
      }
    }
  ]
}